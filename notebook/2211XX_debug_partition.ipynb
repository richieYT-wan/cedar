{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e568d58f-a244-4894-8c8f-e0affd97bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def read_hobohm(filename, original_df, pep_col='Peptide', hla_col='HLA', elrank_col='trueHLA_EL_rank',\n",
    "                target_col='agg_label'):\n",
    "    \"\"\"\n",
    "    Reads the output of a hobohm reduced file\n",
    "    (ex would read \"dataset.out\" from ./hobohm1_pepkernel input.pep > dataset.out\n",
    "    Args:\n",
    "        filename: Filepath to the output file of the hobohm script\n",
    "        original_df: DataFrame or the path to the original dataframe used to run Hobohm\n",
    "        pep_col : Column of DataFrame containing the Peptide\n",
    "        hla_col : Column of DataFrame containing the HLA\n",
    "        elrank_col : Column of DataFrame containing the ELrank\n",
    "    Returns:\n",
    "        unique_df (pd.DataFrame): DataFrame containing the unique sequences\n",
    "        not_unique_df (pd.DataFrame): DataFrame containing the non-unique sequences\n",
    "    \"\"\"\n",
    "    assert type(original_df) in [pd.DataFrame, str], 'original_df must be a Pandas DataFrame or a filepath!'\n",
    "    if type(original_df) != pd.DataFrame:\n",
    "        try:\n",
    "            original_df = pd.read_csv(original_df)\n",
    "        except:\n",
    "            raise ValueError(f\"Couldn't read or access {original_df} of type {type(original_df)}!\")\n",
    "    # Get unique df and merge to original df;\n",
    "    # Given Hobohm, all identical sequences with different HLA should re-appear and will need to be\n",
    "    # addressed later in another function\n",
    "    unique_df = pd.read_csv(filename, header=None, comment='#')\n",
    "    unique_df.columns = [pep_col]\n",
    "    unique_df = unique_df.merge(original_df[[pep_col, hla_col, elrank_col, target_col]],\n",
    "                                left_on=pep_col, right_on=pep_col)\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = [l.strip('\\n') for l in f.readlines()]\n",
    "    # Get the non-unique saved somewhere to re-assign them later\n",
    "    values = []\n",
    "    for l in lines:\n",
    "        if 'Not unique' not in l: continue\n",
    "        l = l.replace('\\n', '')\n",
    "        l = [x.split(' ') for x in l.split('# Not unique. ')[1].split(' is similar to ')]\n",
    "        idx = l[0][0]\n",
    "        discarded = l[0][1]\n",
    "        similar = l[1][0]\n",
    "        similarity_score = l[1][1]\n",
    "        values.append([idx, similar, discarded, similarity_score])\n",
    "    not_unique_df = pd.DataFrame(values, columns=['drop_idx', 'similar', 'discarded', 'similarity_score'])\n",
    "    not_unique_df['self'] = not_unique_df['similar'] == not_unique_df['discarded']\n",
    "    return unique_df, not_unique_df\n",
    "\n",
    "\n",
    "def manually_reassign_identical(k, unique_df, pep_col='Peptide'):\n",
    "    \"\"\"\n",
    "    Assumes the unique_df contains identical sequences with different HLAs due to the previous merge operation.\n",
    "    Assumes the unique_df has already been KFold separated. Then, identical sequences with different HLAs will be reassigned\n",
    "    to the same fold.\n",
    "    Args:\n",
    "        k: K in K fold\n",
    "        unique_df: The unique_df that has previously been kfold split\n",
    "        pep_col: column containing the Peptide\n",
    "\n",
    "    Returns:\n",
    "        unique_df: The unique_df that has undergone re-assignment\n",
    "    \"\"\"\n",
    "\n",
    "    assignment_counts = {x: 0 for x in range(k)}\n",
    "    # Go through all the duplicated peps\n",
    "    for pep in unique_df.loc[unique_df.duplicated(pep_col, keep=False)][pep_col].unique():\n",
    "        tmp = unique_df.loc[unique_df[pep_col] == pep]\n",
    "        # if already all the same fold then it's fine, continue\n",
    "        if len(tmp.fold.unique()) == 1: continue\n",
    "        # otherwise: assign to a fold that has the least assignments (starts at 0)\n",
    "        counts = [assignment_counts[k] for k in tmp.fold.values]\n",
    "        new_assignment = tmp.fold.values[counts.index(min(counts))]\n",
    "        unique_df.loc[unique_df[pep_col] == pep, 'fold'] = new_assignment\n",
    "        assignment_counts[new_assignment] += 1\n",
    "    return unique_df\n",
    "\n",
    "\n",
    "def manually_reassign_related(unique_df, not_unique_df, pep_col='Peptide', hla_col='HLA', elrank_col='trueHLA_EL_rank',\n",
    "                              target_col='agg_label'):\n",
    "    \"\"\"\n",
    "    Manually reassigns non-unique (discarded) peptide to the same fold as their related pep\n",
    "    Args:\n",
    "        unique_df (pd.DataFrame): DataFrame containing the unique sequences\n",
    "        not_unique_df (pd.DataFrame): DataFrame containing the non-unique sequences\n",
    "        pep_col: Column of DataFrame containing the Peptide\n",
    "        hla_col: Column of DataFrame containing the HLA\n",
    "        elrank_col: Column of DataFrame containing the ELrank\n",
    "    Returns:\n",
    "        not_unique-df (pd.DataFrame): Non-unique DataFrame with sequences re-assigned to the correct fold\n",
    "    \"\"\"\n",
    "    # Re-assigning\n",
    "    not_unique_df['fold'] = not_unique_df.apply(lambda x: unique_df.query(f'{pep_col}==@x.similar')['fold'].unique()[0],\n",
    "                                                axis=1)\n",
    "    not_unique_df = not_unique_df[['discarded', target_col, hla_col, elrank_col, 'fold']].rename(\n",
    "        columns={'discarded': pep_col})\n",
    "\n",
    "    return not_unique_df\n",
    "\n",
    "\n",
    "def stratified_kfold_unique(unique_df, not_unique_df, original_df,\n",
    "                     k=5, shuffle=True, seed=13,\n",
    "                     pep_col='Peptide', hla_col='HLA', elrank_col='trueHLA_EL_rank', target_col='agg_label'):\n",
    "    # Stratify KFold on the unique set, based on duplicated counts as stratifying group.\n",
    "    # Then repopulate the folds with\n",
    "    not_unique_df['drop_idx'] = not_unique_df['drop_idx'].astype(int)\n",
    "    # Get the strat KF object\n",
    "    stratkf = StratifiedKFold(k, shuffle=shuffle, random_state=seed)\n",
    "    # Merge the not unique (to get agg_label, i.e. \"y\" for stratkf to split and get the duplicated counts\n",
    "    not_unique_df = not_unique_df.merge(original_df.reset_index()[['index', target_col, hla_col, elrank_col]],\n",
    "                                     left_on='drop_idx', right_on='index')\n",
    "\n",
    "    dup_counts = not_unique_df.groupby('similar').agg({'self': 'count'})\\\n",
    "                              .sort_values('self', ascending=False).reset_index()\n",
    "\n",
    "    dup_counts = dup_counts.merge(unique_df[[pep_col, target_col]], left_on='similar',\n",
    "                                  right_on=pep_col).drop_duplicates(['similar', target_col])\n",
    "    # Merge and assign the duplicated counts, to be used as stratify groups\n",
    "    unique_df['counts'] = 0\n",
    "    tmp = unique_df.reset_index().merge(dup_counts[[pep_col, 'self']], left_on=pep_col, right_on=pep_col)\n",
    "    unique_df.iloc[tmp['index'].values, unique_df.columns.get_loc('counts')] = tmp['self']\n",
    "\n",
    "    # Ready to stratify and set the folds\n",
    "    unique_df['fold'] = np.nan\n",
    "    for i, (train_idx, test_idx) in enumerate(\n",
    "            stratkf.split(unique_df[pep_col].values, unique_df[target_col], groups=unique_df['counts'])):\n",
    "        unique_df.iloc[test_idx, unique_df.columns.get_loc('fold')] = i\n",
    "    unique_df.fold = unique_df.fold.astype(int)\n",
    "    return unique_df, not_unique_df\n",
    "\n",
    "\n",
    "def pipeline_stratified_kfold(hobohm_filename, original_df, k=5, shuffle=True, seed=13,\n",
    "                              pep_col='Peptide', hla_col='HLA', elrank_col='trueHLA_EL_rank', target_col='agg_label'):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        hobohm_filename:\n",
    "        original_df:\n",
    "        k:\n",
    "        shuffle:\n",
    "        seed:\n",
    "        pep_col:\n",
    "        hla_col:\n",
    "        elrank_col:\n",
    "\n",
    "    Returns:\n",
    "        dataset (pd.DataFrame): dataset with assigned folds\n",
    "    \"\"\"\n",
    "    # original_df = original_df.sort_values(pep_col).reset_index(drop=True)\n",
    "    unique_df, not_unique_df = read_hobohm(hobohm_filename, original_df, pep_col, hla_col, elrank_col, target_col)\n",
    "    unique_df, not_unique_df = stratified_kfold_unique(unique_df, not_unique_df, original_df, k, shuffle, seed,\n",
    "                                                pep_col, hla_col, elrank_col, target_col )\n",
    "    unique_df = manually_reassign_identical(k, unique_df, pep_col)\n",
    "    not_unique_df = manually_reassign_related(unique_df, not_unique_df, pep_col, hla_col, elrank_col, target_col)\n",
    "    dataset = pd.concat([unique_df, not_unique_df], ignore_index=True) \\\n",
    "        .sort_values(pep_col, ascending=True).reset_index(drop=True).drop(columns=['counts'])\n",
    "    merge_cols = [pep_col, hla_col]\n",
    "    merge_cols.extend(original_df.columns.difference(dataset.columns))\n",
    "    dataset = dataset.merge(original_df[merge_cols], left_on=[pep_col, hla_col], right_on=[pep_col, hla_col])\n",
    "\n",
    "    # Bugfix: Drops some unique that were duplicated during the manual re-assignment\n",
    "    dataset = dataset.drop(dataset.loc[dataset.duplicated(keep='first')].index)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8958db61-1f3f-4a50-8f7d-91398da9ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('../data/mutant/cedar_prime_concat.csv').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9fdbd-30e1-4405-8b39-6625121e03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df, not_unique_df = read_hobohm(hobohm_filename, original_df, pep_col, hla_col, elrank_col, target_col)\n",
    "unique_df, not_unique_df = stratified_kfold_unique(unique_df, not_unique_df, original_df, k, shuffle, seed,\n",
    "                                            pep_col, hla_col, elrank_col, target_col)\n",
    "unique_df = manually_reassign_identical(k, unique_df, pep_col)\n",
    "not_unique_df = manually_reassign_related(unique_df, not_unique_df, pep_col, hla_col, elrank_col, target_col)\n",
    "dataset = pd.concat([unique_df, not_unique_df], ignore_index=True) \\\n",
    "    .sort_values(pep_col, ascending=True).reset_index(drop=True).drop(columns=['counts'])\n",
    "merge_cols = [pep_col, hla_col]\n",
    "merge_cols.extend(original_df.columns.difference(dataset.columns))\n",
    "dataset = dataset.merge(original_df[merge_cols], left_on=[pep_col, hla_col], right_on=[pep_col, hla_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afdb1fff-3cab-4e0a-8aaf-dedd02a55e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read hobohm 4428 1396\n",
      "strat kfold unique 4428 1396\n",
      "manually reassign identical 4428 1396\n",
      "manually reassign related 4428 1396\n",
      "concat 5824\n",
      "merge 5824\n",
      "drop duplicated 5739\n"
     ]
    }
   ],
   "source": [
    "pep_col = 'Peptide'\n",
    "hla_col = 'HLA'\n",
    "elrank_col = 'EL_rank_mut'\n",
    "target_col = 'agg_label'\n",
    "hobohm_filename = '../../kern_dist/out/cedar_prime_merged.pep_0.9.out'\n",
    "k=5\n",
    "shuffle=True\n",
    "seed=13\n",
    "dataset = pipeline_stratified_kfold(hobohm_filename, original_df, 5, True, 13,\n",
    "                                    pep_col, hla_col, elrank_col, target_col)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f35d333f-5c1c-4028-9b0d-4bba34125484",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df, not_unique_df = stratified_kfold_unique(unique_df, not_unique_df, original_df, k, shuffle, seed,\n",
    "                                            pep_col, hla_col, elrank_col, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a65ba9a-22a6-437a-995f-c401821c2fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5739"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "469a02f7-3384-42e0-9e96-4e0f1286f5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5739"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_ddddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeab97f-e06e-4426-809d-5dc753aaf5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:grl]",
   "language": "python",
   "name": "conda-env-grl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
